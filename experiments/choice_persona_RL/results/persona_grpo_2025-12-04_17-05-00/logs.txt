INFO:research_scaffold.config_tools:========== Config Dict ===========
Config(name='persona_grpo',
       function_name='train_grpo',
       time_stamp_name=True,
       function_kwargs={'dataset_config': {'n_eval': 100},
                        'lora_config': {'bias': 'none',
                                        'lora_alpha': 32,
                                        'lora_dropout': 0.0,
                                        'r': 16,
                                        'target_modules': ['q_proj',
                                                           'k_proj',
                                                           'v_proj',
                                                           'o_proj'],
                                        'task_type': 'CAUSAL_LM'},
                        'model_config': {'attn_implementation': 'eager',
                                         'device_map': 'auto',
                                         'pretrained_model_name_or_path': 'wuschelschulz/gemma-3-1b-persona-ab-sft'},
                        'output_dir': 'results/RUN_NAME',
                        'run_name': 'RUN_NAME',
                        'saving': {'hf': False},
                        'training_config': {'bf16': True,
                                            'eval_on_start': True,
                                            'eval_steps': 50,
                                            'eval_strategy': 'steps',
                                            'gradient_accumulation_steps': 1,
                                            'gradient_checkpointing': True,
                                            'learning_rate': 5e-06,
                                            'logging_steps': 1,
                                            'lr_scheduler_type': 'constant',
                                            'max_completion_length': 64,
                                            'max_prompt_length': 1024,
                                            'max_steps': 500,
                                            'num_generations': 8,
                                            'per_device_eval_batch_size': 32,
                                            'per_device_train_batch_size': 32,
                                            'report_to': 'wandb',
                                            'save_only_model': True,
                                            'save_steps': 100,
                                            'save_total_limit': 3,
                                            'temperature': 1.0}},
       function_args=None,
       log_file_path='results/RUN_NAME/logs.txt',
       save_config_path='results/RUN_NAME/config.yaml',
       wandb_project='adversarial_generalisation',
       wandb_entity=None,
       wandb_group='persona_grpo',
       wandb_tags=None,
       instance=None)
INFO:research_scaffold.config_tools:Run Name: 'persona_grpo_2025-12-04_17-05-00'
INFO:research_scaffold.config_tools:Saved config to: results/persona_grpo_2025-12-04_17-05-00/config.yaml
INFO:functions.train_grpo:Starting GRPO training for run: persona_grpo_2025-12-04_17-05-00
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:functions.train_grpo:Loaded 2251 training samples and 100 eval samples
